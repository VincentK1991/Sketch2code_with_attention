# -*- coding: utf-8 -*-
"""Decoder.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i7RfPr9auJWxuJ6zWK_p3Q7FGoNoInd2
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd '/content/drive/My Drive/Colab Notebooks/Image2codes'

import torch
from torch import nn
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class Attention(nn.Module):
  """
  Attention network
  """
  def __init__(self,encoder_dim,decoder_dim,attention_dim):
    """
    param encoder_dim = feature size of encoded images
    param decoder_dim = size of decoder's RNN
    param attention_dim = size of the attention network
    """
    super(Attention,self).__init__()
    self.encoder_att = nn.Linear(encoder_dim,attention_dim)
    self.decoder_att = nn.Linear(decoder_dim,attention_dim)
    self.full_att = nn.Linear(attention_dim,1)
    self.relu = nn.ReLU()
    self.softmax = nn.Softmax(dim=1)

  def forward(self,encoder_out,decoder_hidden):
    """
    forward propagation
    encoder_out = encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)
    decoder_hidden = previous decoder output, a tensor of dimension (batch_size, decoder_gim)
    return: attention weighted encoding, weights
    """

    att1 = self.encoder_att(encoder_out) #(batch_size,num_pixels,attention_dim)
    att2 = self.decoder_att(decoder_hidden) # batch_size, attention_dim
    att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2) 
    alpha = self.softmax(att) # batch_size, num_pixels
    attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1) # (batch_size,encoder_dim)

    return attention_weighted_encoding, alpha

class Decoder(nn.Module):
  """
  Decoder
  """
  def __init__(self,attention_dim,embed_dim,decoder_dim,vocab_size,encoder_dim=2048,dropout=0.5):
    """
    attention_dim = size of the attention network
    embed_dim = embedding_size
    decoder_dim = size of decoder's RNN
    vocab_size = size of the vocab
    encoder_dim = feature size of encoded images
    dropout = probability of drop out
    """
    super(Decoder,self).__init__()

    self.encoder_dim = encoder_dim
    self.attention_dim = attention_dim
    self.embed_dim = embed_dim
    self.decoder_dim = decoder_dim
    self.vocab_size = vocab_size
    self.dropout_p = dropout

    self.attention = Attention(encoder_dim,decoder_dim,attention_dim)

    self.embedding = nn.Embedding(vocab_size,embed_dim) # embedding layer
    self.dropout = nn.Dropout(p=self.dropout_p)
    self.decode_step = nn.LSTMCell(embed_dim + encoder_dim,decoder_dim,bias=True)
    self.init_h = nn.Linear(encoder_dim,decoder_dim) # linear layer to find initial hidden state of LSTMCell
    self.init_c = nn.Linear(encoder_dim,decoder_dim) # linear layer to find initial cell state of LSTMCell

    self.f_beta = nn.Linear(decoder_dim,encoder_dim) # linear layer to create a sigmoid-activated gate
    self.sigmoid = nn.Sigmoid()
    self.fc = nn.Linear(decoder_dim,vocab_size) # fully connected linear layer to find the scores over vocab
    self.init_weights() # initialize some layers with the uniform distribution

  def init_weights(self):
    """
    initialize some parameters with values from the uniform distribution
    """
    self.embedding.weight.data.uniform_(-0.1,0.1)
    self.fc.bias.data.fill_(0)
    self.fc.weight.data.uniform_(-0.1,0.1)

  def init_hidden_state(self,encoder_out):
    """
    create the initial hidden and cell states for the decoder's LSTM based on the encoded images.

    encoder_out = encoded images, a tensor of dimension (batch_size,num_pixels, encoder_dim)

    return hidden state, cell state
    """

    mean_encoder_out = encoder_out.mean(dim=1)
    h = self.init_h(mean_encoder_out)
    c = self.init_c(mean_encoder_out)
    return h,c

  def forward(self,encoder_out,encoded_captions,caption_lengths):
    """
    encoder_out = encoded images, a tensor of dim (batch_size, enc_image_size,enc_image_size,encoder_dim)
    encoded_caption = a tokenized caption before embedding tensor of dimension (batch_size, max_caption_length)
    caption_lengths = a tensor of dimension (batch_size,1)  # exclude padding

    return scores for vocab,
    """
    batch_size = encoder_out.size(0)
    encoder_dim = encoder_out.size(-1)
    vocab_size = self.vocab_size

    # flatten image
    encoder_out = encoder_out.view(batch_size,-1,encoder_dim) # batch_size,num_pixels, encoder_dim
    num_pixels = encoder_out.size(1)
    # Sort input data by decreasing lengths; why? apparent below

    caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)
    encoder_out = encoder_out[sort_ind]
    encoded_captions = encoded_captions[sort_ind]
    # embedding
    embeddings = self.embedding(encoded_captions)

    # initialize LSTM state
    h,c = self.init_hidden_state(encoder_out)

    # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>
    # So, decoding lengths are actual lengths - 1
    decode_lengths = (caption_lengths - 1).tolist()
    # creatte tensor to hold word prediction scores and alphas
    predictions = torch.zeros(batch_size,max(decode_lengths),vocab_size).to(device)
    alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)

    # at each time step, decode by
    # attention-weighting the encoder's output based on the decoder's previous hidden state output
    # then generate a new word in the decoder with previous word, and the attention weighted encoding

    for t in range(max(decode_lengths)):
      batch_size_t = sum([l > t for l in decode_lengths])
      attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],
                                                          h[:batch_size_t])
      
      gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)
      attention_weighted_encoding = gate * attention_weighted_encoding
      h, c = self.decode_step(
          torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),
          (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)
      preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)
      predictions[:batch_size_t, t, :] = preds
      alphas[:batch_size_t, t, :] = alpha

    return predictions, encoded_captions, decode_lengths, alphas, sort_ind

  def generate(self,encoder_out,word_map,beam_size=3):
    """
    encoder_out = encoded images, a tensor of dim (batch_size, enc_image_size,enc_image_size,encoder_dim)
    word_map = word_map (dictionary from word to index)
    beam_size = number of sequences to consider at each decode step
    return a list of tokenized output (from highest probability beam search)
    """
    k = beam_size
    vocab_size = len(word_map)

    hypothesis = []

    enc_image_size = encoder_out.size(1)
    encoder_dim = encoder_out.size(3)
    encoder_out = encoder_out.view(1, -1, encoder_dim)
    num_pixels = encoder_out.size(1)

    encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)

    k_prev_words = torch.LongTensor([[word_map['<BOS>']]] * k).to(device)  # (k, 1)
    seqs = k_prev_words  # (k, 1)
    top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)
    seqs_alpha = torch.ones(k, 1, enc_image_size, enc_image_size).to(device)

    complete_seqs = list()
    complete_seqs_alpha = list()
    complete_seqs_scores = list()

    step = 1
    h, c = self.init_hidden_state(encoder_out)
    while True:
      embeddings = self.embedding(k_prev_words).squeeze(1)
      awe, alpha = self.attention(encoder_out, h) 
      alpha = alpha.view(-1, enc_image_size, enc_image_size)
      gate = self.sigmoid(self.f_beta(h))
      awe = gate * awe

      h, c = self.decode_step(torch.cat([embeddings, awe], dim=1), (h, c)) 

      scores = self.fc(h)
      scores = nn.functional.log_softmax(scores, dim=1)
      scores = top_k_scores.expand_as(scores) + scores

      if step == 1:
        top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)
      else:
        top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)

      prev_word_inds = top_k_words / vocab_size  # (s)
      next_word_inds = top_k_words % vocab_size  # (s)

      seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)
      seqs_alpha = torch.cat([seqs_alpha[prev_word_inds], alpha[prev_word_inds].unsqueeze(1)],dim=1)

      incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if
                          next_word != word_map['<EOS>']]
      complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))

      if len(complete_inds) > 0:
          complete_seqs.extend(seqs[complete_inds].tolist())
          complete_seqs_alpha.extend(seqs_alpha[complete_inds].tolist())
          complete_seqs_scores.extend(top_k_scores[complete_inds])

      k -= len(complete_inds)  # reduce beam length accordingly

      if k == 0:
          break
      seqs = seqs[incomplete_inds]
      seqs_alpha = seqs_alpha[incomplete_inds]
      h = h[prev_word_inds[incomplete_inds]]
      c = c[prev_word_inds[incomplete_inds]]
      encoder_out = encoder_out[prev_word_inds[incomplete_inds]]
      top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)
      k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)

      if step > 51:
          print(' break beam-search loop after searching for 51 steps')
          if complete_seqs_scores == []:
            #print(' return all incomplete search as complete ')
            complete_seqs.extend(seqs[incomplete_inds].tolist())
            complete_seqs_alpha.extend(seqs_alpha[incomplete_inds].tolist())
            complete_seqs_scores.extend(top_k_scores[incomplete_inds])
          break
      step += 1
    i = complete_seqs_scores.index(max(complete_seqs_scores))
    beam_search_seq = complete_seqs[i]  # a list of tokens
    alphas = complete_seqs_alpha[i]

    return beam_search_seq