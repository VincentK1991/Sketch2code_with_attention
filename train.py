# -*- coding: utf-8 -*-
"""train_data_main.ipynb

Automatically generated by Colaboratory.

"""

import argparse, json

import time
import timeit
import torch.backends.cudnn as cudnn
import torch.optim
import torch.utils.data
import torchvision.transforms as transforms
from torch import nn
from torch.nn.utils.rnn import pack_padded_sequence
import pandas as pd
import numpy as np
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset, RandomSampler
from torch.utils.tensorboard import SummaryWriter
cudnn.benchmark = True
print('cudnn is True')
from model import encoder, decoder

with open('token_dir/line2idx.json', 'r') as f:
    line2idx = json.load(f)

idx2line = {}
for item in line2idx.keys():
  idx2line[line2idx[item]] = item
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def main(args):
  writer = SummaryWriter()
  encoder_model = encoder.Encoder(args.encoded_img_size)
  decoder_model = decoder.Decoder(attention_dim=args.attention_dim,
                                embed_dim=args.word_embedding_dim,
                                decoder_dim=args.decoder_dim,
                                vocab_size=len(line2idx),
                                dropout=args.dropout)

  encoder_model.fine_tune(True)
  decoder_model = decoder_model.to(device)
  encoder_model = encoder_model.to(device)

  criterion = nn.CrossEntropyLoss().to(device)
  try:
    state_dict_encoder = torch.load(args.encoder_path)
    encoder_model.load_state_dict(state_dict_encoder)
  except:
  	print('pre-trained encoder model not provided')
  try:
    state_dict_decoder = torch.load(args.decoder_path)
    decoder_model.load_state_dict(state_dict_decoder)
  except:
  	print('pre-trained decoder model not provided')

  encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder_model.parameters()),
                    lr=args.encoder_lr) if args.fine_tune_encoder else None

  decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder_model.parameters()),
                                lr=args.decoder_lr)
  
  train_dataset = torch.load(args.train_data)
  train_sampler = RandomSampler(train_dataset)
  train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=1)
  val_dataset = torch.load(args.val_data)
  val_sampler = RandomSampler(val_dataset)
  val_dataloader = DataLoader(val_dataset, sampler=val_sampler, batch_size=1)
  print('--finished loading train & validation data--')
  start = timeit.default_timer()

  for epoch in range(args.epochs):
    train_temp_loss = []
    for count,item in enumerate(train_dataloader):
      train_loss = train(encoder_model,decoder_model,encoder_optimizer,decoder_optimizer,criterion,line2idx,item,count,args.alpha_c,args.grad_clip,args.gradient_accumulation)
      train_temp_loss.append(train_loss)

    if epoch % args.print_every == 0:
      print('train loss at epoch ',epoch,' = ',np.mean(train_temp_loss))
    if epoch % args.val_every == 0:
      val_temp_accu = []
      for item in val_dataloader:
        hypotheses, accu = validate_no_teacher_forcing(encoder_model,decoder_model,line2idx,item)
        val_temp_accu.append(accu)

      print('validation accuracy scores = ',np.mean(val_temp_accu))
      stop = timeit.default_timer()
      print('epochs ', epoch,' takes ',stop - start, ' sec')
      print(' ')
      start = timeit.default_timer()

    writer.add_scalar('train_loss', np.mean(train_temp_loss), epoch)
    writer.add_scalar('val_acc',np.mean(val_temp_accu), epoch)
  
  # save model after finish the epochs
  encoder_file = 'result_dir/encoder_' + args.model_name+ '_' + str(args.epochs) + '.pt'
  torch.save(encoder_model.state_dict(), encoder_file)
  decoder_file = 'result_dir/decoder_' + args.model_name+ '_' + str(args.epochs) + '.pt'
  torch.save(decoder_model.state_dict(), decoder_file)
  print('Saved model to ' + encoder_file + ' and ' + decoder_file)
  writer.close()

def train(encoder_model,decoder_model,encoder_optimizer,decoder_optimizer,criterion,line2idx,batch,count,alpha_c=1.0,grad_clip=3.0,gradient_accumulation_steps=5):
  """ 
  train 1 batch of input. Use teacher forcing.
  each batch has strictly 1 sample (i.e. batch of 1)
  use gradient accumulation to accumulate the gradient before optimizer step
  
  return: the loss and the accuracy 
  """
  imgs = batch[0]#.unsqueeze(0)
  caps = batch[1]#.unsqueeze(0)
  caplens = batch[2] #.unsqueeze(0)

  decoder_model.train()  # train mode (dropout and batchnorm is used)
  encoder_model.train()

  start = time.time()

  # Move to GPU, if available
  imgs = imgs.to(device)
  caps = caps.to(device)
  caplens = caplens.to(device)

  # Forward prop.
  imgs = encoder_model(imgs)
  scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder_model(imgs, caps, caplens)

  # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>
  targets = caps_sorted[:, 1:]

  scores,_,_,_ = pack_padded_sequence(scores, decode_lengths, batch_first=True)
  targets,_,_,_ = pack_padded_sequence(targets, decode_lengths, batch_first=True)
  loss = criterion(scores, targets)

  # Add doubly stochastic attention regularization for soft attention
  # penalize the model so it tends to use entire image for attention
  loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()
  loss = loss/gradient_accumulation_steps
  # Back prop.
  loss.backward()
  torch.nn.utils.clip_grad_norm_(decoder_model.parameters(), grad_clip)
  torch.nn.utils.clip_grad_norm_(encoder_model.parameters(), grad_clip)

  if count % gradient_accumulation_steps == 0:
    decoder_optimizer.step()
    encoder_optimizer.step()

    decoder_optimizer.zero_grad()
    encoder_optimizer.zero_grad()

  return loss.item()

def validate_no_teacher_forcing(encoder_model,decoder_model,line2idx,batch,beam_size=3):
  """
  Perform validation of 1 batch input. This validation does not use teacher forcing.
  Thus the generation uses beam-search.
  each batch has strictly 1 sample (i.e. batch of 1)

  return the beam search result (one with highest probability), and accuracy
  """
  decoder_model.eval()  # eval mode (no dropout or batchnorm)
  encoder_model.eval()

  imgs = batch[0]#.unsqueeze(0)
  caps = batch[1]#.unsqueeze(0)
  caplens = batch[2] #.unsqueeze(0)
  k = beam_size
  vocab_size = len(line2idx)
  references = list()  # references (true captions) for calculating BLEU-4 score
  hypotheses = list()  # hypotheses (predictions)
  with torch.no_grad():
    imgs = imgs.to(device)
    caps = caps.to(device)
    caplens = caplens.to(device)

    encoder_out = encoder_model(imgs)
    beam_search_seq = decoder_model.generate(encoder_out,line2idx,beam_size)
  
  allcaps = caps
  for j in range(allcaps.shape[0]):

    img_caps = allcaps[j].tolist()

    img_captions = []
    for w in img_caps:
      if w not in [line2idx['<PAD>']]:
      #if w not in [line2idx['<EOS>'],line2idx['<BOS>'], line2idx['<PAD>']]:
        img_captions.append(w)
    references.append(img_captions)

  accuracy = np.sum(np.where(np.array(beam_search_seq) == np.array(references[0]),1,0))/len(references[0])

  return beam_search_seq,accuracy

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='sketch2code with attention')
    parser.add_argument('--epochs', type=int, default=5, metavar='E',
                        help='number of epochs to train for (default: 5)')
    
    
    parser.add_argument('--encoder_lr', type=float, default=5e-5, metavar='en_LR',
                        help='learning rate of the encoder (default: 5e-5)')
    parser.add_argument('--decoder_lr', type=float, default=5e-5, metavar='de_LR',
                        help='learning rate of the decoder (default: 5e-5)')
    parser.add_argument('--alpha_c', type=float, default=1.0, metavar='A',
                        help='regularization constant (default: 1)')
    parser.add_argument('--dropout', type=float, default=0.5,
                        help='dropout probability (default:0.5)')
    parser.add_argument('--grad_clip', type=float, default=3.0,
                        help='gradient norm clipping (default:3.0)')
    parser.add_argument('--gradient_accumulation', type=int, default=5,
                        help='gradient accumulation (since batch size not implemented) (default:5)')
    

    parser.add_argument('--train_data', type=str,help='path to train tensor data .pt')
    parser.add_argument('--val_data', type=str,help='path to validation tensor data .pt')
    parser.add_argument('--model_name', type=str, default='resnet101',
                        help='name of the network to be saved (default: resnet101)')
    
    parser.add_argument('--encoded_img_size',type=int,default=14,
                        help = 'dimension of the encoded output is nxn')
    parser.add_argument('--word_embedding_dim',type=int,default=256,
                        help = 'dimension of the word embedding')
    parser.add_argument('--attention_dim',type=int,default=256,
                        help = 'dimension of the attention head')
    parser.add_argument('--decoder_dim',type=int,default=256,
                        help = 'dimension of the decoder')
    
    parser.add_argument('--encoder_path', type=str, help='path to the encoder model')
    parser.add_argument('--decoder_path', type=str, help='path to the decoder model')
    parser.add_argument('--fine_tune_encoder', default=True,
                        help='fine-tune the encoder CNN model during training (default: True)')
    parser.add_argument('--print_every', type=int,default=1, help='print training result every X epoch')
    parser.add_argument('--val_every', type=int,default=1, help='print validation result every X epoch')
    main(parser.parse_args())